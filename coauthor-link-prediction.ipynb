{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-author Network Link Prediction - Tuning Node2vec model\n",
    "\n",
    "https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 - Binary Operator\n",
    "\n",
    "https://stellargraph.readthedocs.io/en/stable/demos/link-prediction/node2vec-link-prediction.html#refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be rewritten to take in the function as an argument\n",
    "def binary_operator(name,u,v):\n",
    "    \n",
    "    def operator_hadamard(u, v):\n",
    "        return u * v\n",
    "\n",
    "    def operator_l1(u, v):\n",
    "        return np.abs(u - v)\n",
    "\n",
    "    def operator_l2(u, v):\n",
    "        return (u - v) ** 2\n",
    "\n",
    "    def operator_avg(u, v):\n",
    "        return (u + v) / 2.0\n",
    "\n",
    "    if name==\"hadamard\":\n",
    "        return operator_hadamard(u,v)\n",
    "    elif name==\"l1\":\n",
    "        return operator_l1(u,v)\n",
    "    elif name==\"average\":\n",
    "        return operator_avg(u,v)\n",
    "    elif name==\"l2\":\n",
    "        return operator_l2(u,v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Coauthorship Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://neo4j.com/blog/cypher-load-json-from-url/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import  Graph, Node\n",
    "\n",
    "graphdb = Graph(scheme=\"bolt\", host=\"localhost\", port=7687, secure=False, auth=('neo4j', 'test'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8d3513b7698b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"link_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"link_prediction\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "test_df=pd.read_csv(\"data/testdf.csv\")\n",
    "training_df=pd.read_csv(\"data/trainingdf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=spark.createDataFrame(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training_data = training_data.repartition(2000)\n",
    "#training_data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|node1|node2|label|\n",
      "+-----+-----+-----+\n",
      "| 2625| 2628|    1|\n",
      "| 2175| 2175|    0|\n",
      "|  115|  116|    1|\n",
      "| 1542| 1543|    1|\n",
      "| 1994| 1998|    1|\n",
      "+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 2400|\n",
      "|    1| 2400|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training_data.groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=spark.createDataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_data = training_data.repartition(2000)\n",
    "#test_data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py2neo.database.Cursor at 0x7fc04beab110>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a graph projection\n",
    "graphdb.run(\"\"\"CALL gds.graph.create('early_graph',\n",
    "    'Author', \n",
    "    {\n",
    "        CO_AUTHOR_EARLY: {\n",
    "                type: 'CO_AUTHOR_EARLY',\n",
    "                orientation: 'UNDIRECTED'\n",
    "                }\n",
    "                }\n",
    "                )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'early_graph'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphdb.run(\"\"\"CALL gds.graph.list\"\"\").evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py2neo.database.Cursor at 0x7fc04b28ec50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a graph projection\n",
    "graphdb.run(\"\"\"CALL gds.graph.create('late_graph',\n",
    "    'Author', \n",
    "    {\n",
    "        CO_AUTHOR: {\n",
    "                type: 'CO_AUTHOR',\n",
    "                orientation: 'UNDIRECTED'\n",
    "                }\n",
    "                }\n",
    "                )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason doesnt have this algorithm\n",
    "# graphdb.run(\"\"\"CALL gds.alpha.node2vec.stream('early_graph', {dimensions: 2})\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import StructField, StructType,IntegerType\n",
    "\n",
    "def apply_node2vec_features(data, graph_name, walk_length, num_walks, dimensions, \n",
    "                            window_size, p, q, num_iter, workers, operator_name,\n",
    "                            output_col_name):\n",
    "    \n",
    "    params = {\n",
    "    \"pairs\": [{\"node1\": row[\"node1\"], \"node2\": row[\"node2\"]}\n",
    "    for row in data.collect()],\n",
    "    \"steps\": walk_length,\n",
    "    \"walks\": num_walks,\n",
    "    \"size\": dimensions,\n",
    "    \"graph_name\": graph_name\n",
    "    }\n",
    "\n",
    "    query=(\"\"\"\n",
    "    UNWIND $pairs as pair\n",
    "    MATCH (p:Author) WHERE id(p) = pair.node1 OR id(p) = pair.node2\n",
    "    WITH DISTINCT p\n",
    "    CALL gds.alpha.randomWalk.stream($graph_name,{\n",
    "        start: id(p),\n",
    "        steps: $steps,\n",
    "        walks: $walks\n",
    "    })\n",
    "    YIELD nodeIds\n",
    "    RETURN [id in nodeIds | toString(id)] as walks\n",
    "    \"\"\")\n",
    "\n",
    "    random_walks=graphdb.run(query, params).to_series()\n",
    "    \n",
    "    model=gensim.models.Word2Vec(random_walks, sg=1, window=window_size, size=dimensions, min_count=1,\n",
    "                                 workers=workers,iter=num_iter)\n",
    "\n",
    "    vectors=[{\"node1\":row[\"node1\"],\n",
    "            \"node2\": row[\"node2\"],\n",
    "            output_col_name: Vectors.dense(\n",
    "                binary_operator(operator_name, model.wv[str(row[\"node1\"])], model.wv[str(row[\"node2\"])]))\n",
    "            } for row in data.collect()]\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField('node1', IntegerType()),\n",
    "        StructField('node2', IntegerType()),\n",
    "        StructField(output_col_name, VectorUDT())])\n",
    "\n",
    "    features=spark.createDataFrame(vectors, schema)\n",
    "    return data.join(features, [\"node1\", \"node2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1.0\n",
    "q = 1.0\n",
    "dimensions = 128\n",
    "num_walks = 10\n",
    "walk_length = 80\n",
    "window_size = 10\n",
    "num_iter = 1\n",
    "workers = 2\n",
    "operator_name=\"hadamard\"\n",
    "training_data = apply_node2vec_features(training_data, 'early_graph', walk_length, num_walks, dimensions, \n",
    "                            window_size, p, q, num_iter, workers, operator_name,\n",
    "                            \"hadamard_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_name=\"average\"\n",
    "training_data = apply_node2vec_features(training_data, 'early_graph', walk_length, num_walks, dimensions, \n",
    "                            window_size, p, q, num_iter, workers, operator_name,\n",
    "                            \"average_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_name=\"l1\"\n",
    "training_data = apply_node2vec_features(training_data, 'early_graph', walk_length, num_walks, dimensions, \n",
    "                            window_size, p, q, num_iter, workers, operator_name,\n",
    "                            \"l1_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_name=\"l2\"\n",
    "training_data = apply_node2vec_features(training_data, 'early_graph', walk_length, num_walks, dimensions, \n",
    "                            window_size, p, q, num_iter, workers, operator_name,\n",
    "                            \"l2_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_name=\"hadamard\"\n",
    "test_data = apply_node2vec_features(test_data, 'late_graph', walk_length, num_walks, dimensions, \n",
    "                            window_size, p, q, num_iter, workers, operator_name,\n",
    "                            \"hadamard_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_name=\"average\"\n",
    "test_data = apply_node2vec_features(test_data, 'late_graph', walk_length, num_walks, dimensions, \n",
    "                            window_size, p, q, num_iter, workers, operator_name,\n",
    "                            \"average_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_name=\"l1\"\n",
    "test_data = apply_node2vec_features(test_data, 'late_graph', walk_length, num_walks, dimensions, \n",
    "                            window_size, p, q, num_iter, workers, operator_name,\n",
    "                            \"l1_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_name=\"l2\"\n",
    "test_data = apply_node2vec_features(test_data, 'late_graph', walk_length, num_walks, dimensions, \n",
    "                            window_size, p, q, num_iter, workers, operator_name,\n",
    "                            \"l2_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='RFormula_9b6d36284952', name='formula', doc='R model formula'): 'label ~ hadamard_model'},\n",
       " {Param(parent='RFormula_9b6d36284952', name='formula', doc='R model formula'): 'label ~ average_model'},\n",
       " {Param(parent='RFormula_9b6d36284952', name='formula', doc='R model formula'): 'label ~ l1_model'},\n",
       " {Param(parent='RFormula_9b6d36284952', name='formula', doc='R model formula'): 'label ~ l2_model'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "rForm = RFormula()\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "params = ParamGridBuilder()\\\n",
    "    .addGrid(rForm.formula, [\n",
    "    \"label ~ hadamard_model\",\n",
    "    \"label ~ average_model\",\n",
    "    \"label ~ l1_model\",\n",
    "    \"label ~ l2_model\"])\\\n",
    "    .build()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \n",
    "        featuresCol=\"features\",\n",
    "        numTrees=30, maxDepth=10)\n",
    "\n",
    "stages=[rForm, rf]\n",
    "pipeline=Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "    .setMetricName(\"areaUnderROC\")\\\n",
    "    .setRawPredictionCol(\"prediction\")\\\n",
    "    .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mycrossvalidator' from '/home/areias/Documents/DataScience/graphs/mycrossvalidator.py'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mycrossvalidator as mycv\n",
    "from importlib import reload  \n",
    "reload(mycv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = mycv.MyCrossValidator(parallelism=2)\\\n",
    "    .setEstimator(pipeline)\\\n",
    "    .setEvaluator(evaluator)\\\n",
    "    .setEstimatorParamMaps(params)\\\n",
    "    .setCollectSubModels(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data.withColumn(\"fold\",lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data.withColumn(\"test\",lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=test_data.withColumn(\"fold\",lit(0))\n",
    "test_data=test_data.withColumn(\"test\",lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df=training_data.union(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycvfitted, foldstats = cv.fit(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycvfitted.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "\n",
    "def get_roc_curve(model,test_data):\n",
    "    \n",
    "    predictions=model.transform(test_data)\n",
    "    \n",
    "    preds = predictions.select('label','probability')\\\n",
    "    .rdd.map(lambda row: (float(row['probability'][1]), float(row['label'])))\\\n",
    "    .collect()\n",
    "     \n",
    "    y_score, y_true = zip(*preds)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label = 1)\n",
    "    \n",
    "    return fpr,tpr,thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_h,tpr_h,thresholds_h = get_roc_curve(mycvfitted.subModels[0][0], test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_a,tpr_a,thresholds_a = get_roc_curve(mycvfitted.subModels[0][1], test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_l1,tpr_l1,thresholds_l1 = get_roc_curve(mycvfitted.subModels[0][2], test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_l2,tpr_l2,thresholds_l2 = get_roc_curve(mycvfitted.subModels[0][3], test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycler\n",
    "def create_roc_plot():\n",
    "    plt.style.use('classic')\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.rc('axes', prop_cycle=(cycler('color',\n",
    "    ['r', 'g', 'b', 'c', 'm', 'y', 'k'])))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Random score (AUC = 0.50)')\n",
    "    return plt, fig\n",
    "\n",
    "def add_curve(plt, title, fpr, tpr, roc):\n",
    "    plt.plot(fpr, tpr, label=f\"{title} (AUC = {roc:0.2})\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt,fig = create_roc_plot()\n",
    "\n",
    "add_curve(plt, \"Hadamard\",\n",
    "    fpr_h, tpr_h,\n",
    "    mycvfitted.avgMetrics[0])\n",
    "\n",
    "add_curve(plt, \"Average\",\n",
    "    fpr_a, tpr_a,\n",
    "    mycvfitted.avgMetrics[1])\n",
    "\n",
    "add_curve(plt, \"L1\",\n",
    "    fpr_l1, tpr_l1,\n",
    "    mycvfitted.avgMetrics[2])\n",
    "\n",
    "add_curve(plt, \"L2\",\n",
    "    fpr_l2, tpr_l2,\n",
    "    mycvfitted.avgMetrics[3])\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 - Return parameter *p* and In-out parameter *q*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|node1|node2|label|              model0|              model1|              model2|              model3|              model4|              model5|              model6|              model7|              model8|              model9|             model10|             model11|             model12|             model13|             model14|             model15|\n",
      "+-----+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   22|   25|    1|[0.05947017669677...|[0.08510118722915...|[0.07525193691253...|[0.04050952196121...|[0.04371535778045...|[0.00769299268722...|[0.05433988571166...|[0.02938020229339...|[0.03744214773178...|[0.03451131284236...|[0.05300061032176...|[0.02077564597129...|[0.05522674322128...|[0.00495728850364...|[0.01747357845306...|[0.04493346810340...|\n",
      "+-----+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data.drop(\"hadamard_model\", \"average_model\", \"l1_model\", \"l2_model\")\n",
    "test_data=test_data.drop(\"hadamard_model\", \"average_model\", \"l1_model\", \"l2_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = 12\n",
    "num_walks = 10\n",
    "walk_length = 80\n",
    "window_size = 10\n",
    "num_iter = 1\n",
    "workers = 2\n",
    "operator_name=\"l1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'p': 0.25, 'q': 0.25}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "grid = [{'p': [0.25],'q': [0.25]}]\n",
    "list(ParameterGrid(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for item in enumerate(ParameterGrid(grid)):\n",
    "    model_name=\"model\"+str(item[0])\n",
    "    training_data = apply_node2vec_features(training_data, 'early_graph', walk_length, num_walks, dimensions, \n",
    "                            window_size, item[1]['p'], item[1]['q'], num_iter, workers, operator_name,\n",
    "                            model_name)\n",
    "    test_data = apply_node2vec_features(test_data, 'late_graph', walk_length, num_walks, dimensions, \n",
    "                            window_size, item[1]['p'], item[1]['q'], num_iter, workers, operator_name,\n",
    "                            model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label ~ model0']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"label ~ model\"+str(i) for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='RFormula_9b6d36284952', name='formula', doc='R model formula'): 'label ~ model0'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = ParamGridBuilder()\\\n",
    "    .addGrid(rForm.formula, [\"label ~ model\"+str(i) for i in range(1)])\\\n",
    "    .build()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data.withColumn(\"fold\",lit(0))\n",
    "training_data=training_data.withColumn(\"test\",lit(0))\n",
    "\n",
    "test_data=test_data.withColumn(\"fold\",lit(0))\n",
    "test_data=test_data.withColumn(\"test\",lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df=training_data.union(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+----+----+--------------------+\n",
      "|node1|node2|label|fold|test|              model0|\n",
      "+-----+-----+-----+----+----+--------------------+\n",
      "|   22|   25|    1|   0|   0|[0.11191907525062...|\n",
      "|  412| 2326|    0|   0|   0|[0.02781665325164...|\n",
      "+-----+-----+-----+----+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_df.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_df = all_df.repartition(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = mycv.MyCrossValidator()\\\n",
    "    .setEstimator(pipeline)\\\n",
    "    .setEvaluator(evaluator)\\\n",
    "    .setEstimatorParamMaps(params)\\\n",
    "    .setCollectSubModels(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycvfitted, foldstats = cv.fit(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8271662041319765]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycvfitted.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8271662041319765]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/areias/Documents/DataScience/graphs/results.pkl', 'wb') as f:\n",
    "    pickle.dump(mycvfitted.avgMetrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_3abf13843c5f"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycvfitted.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neo4j]",
   "language": "python",
   "name": "conda-env-neo4j-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
